---
title: "Assignment 6_EZ"
author: "Elyse Zhang"
date: "5/3/2018"
output: html_document
---

## 1. Separate mixed samples of linear model data using logistic regression
In addition to the main sample ResidualAnalysisProjectData_1.csv you are given the training sample  ResidualAnalysisProjectData_1_Train.csv which indicates by which of the two mixed models each observation is generated (see column 3). Use this information to separate models in the main sample.

### 1.1. Analize the training sample
Read the training data.
```{r}
datapath <- "~/Documents/UChicago/Courses/Statistical Analysis/Assignments/Week 6/"
LinearModel.Training <- read.csv(file=paste(datapath,'ResidualAnalysisProjectData_1_Train.csv',sep="/"),
                               header=TRUE,sep=",")
nSample.Training<-length(LinearModel.Training[,1])
head(LinearModel.Training)

```

Plot the training sample
```{r}
plot(LinearModel.Training[,1],LinearModel.Training[,2], type="p",pch=19)
```

Then separate the models in the training sample using the third column and plot the training subsamples:
```{r}
# Define training samples generated by model 1 and model 2
LinearModel.Training.1<-cbind(LinearModel.Training[,1],rep(NA,nSample.Training))
LinearModel.Training.2<-cbind(LinearModel.Training[,1],rep(NA,nSample.Training))
LinearModel.Training.1[LinearModel.Training[,3]*(1:nSample.Training),2]<-
  LinearModel.Training[LinearModel.Training[,3]*(1:nSample.Training),2]
LinearModel.Training.2[(1-LinearModel.Training[,3])*(1:nSample.Training),2]<-
  LinearModel.Training[(1-LinearModel.Training[,3])*(1:nSample.Training),2]

head(cbind(LinearModel.Training,
           Trainig1=LinearModel.Training.1[,2],
           Training2=LinearModel.Training.2[,2]))

# Plot the subsamples
matplot(LinearModel.Training[,1],cbind(LinearModel.Training.1[,2],LinearModel.Training.2[,2]),
        pch=16,col=c("green","blue"),ylab="Subsamples of the training sample")

```

Like in the project on analysis of residuals with ResidualAnalysisProjectData_1.csv estimate linear model for the training sample using function lm, look at the output of the function.
```{r}
EstimatedLinearModel.Training = lm(LinearModel.Training[,2] ~ LinearModel.Training[,1])

summary(EstimatedLinearModel.Training)$coefficients

summary(EstimatedLinearModel.Training)$r.squared

summary(EstimatedLinearModel.Training)$sigma

```

**Interpret the results in the output. Compare the results for the trainig sample and the main sample from previous week: coefficients, R2, σ.**
**Main sample's sigma is 0.838893, R-squared:  0.8308, meaning that training sample has less variance of residuals, but model also explains less variance than if it were for the main sample** 

Observe the residuals of the training sample.
```{r}
EstimatedResiduals.Training<-EstimatedLinearModel.Training$residuals
plot(LinearModel.Training[,1],EstimatedResiduals.Training)
```


Using the third column of the training sample separate and plot the residuals for observations from different models
```{r}
# Define residuals corresponding to different models 
EstimatedResiduals.Training.1<-EstimatedResiduals.Training
EstimatedResiduals.Training.2<-EstimatedResiduals.Training
EstimatedResiduals.Training.1[(LinearModel.Training[,3]==0)*(1:nSample.Training)]<-NA
EstimatedResiduals.Training.2[(LinearModel.Training[,3]==1)*(1:nSample.Training)]<-NA
# Print the first ten columns to check the separation 
head(cbind(AllResiduals=EstimatedResiduals.Training,
      Training1Residuals=EstimatedResiduals.Training.1,
      Training2Residuals=EstimatedResiduals.Training.2,
      TrainingClass=LinearModel.Training[,3]))

# Plot the residuals corresponding to different models
matplot(LinearModel.Training[,1],cbind(EstimatedResiduals.Training.1,
                                       EstimatedResiduals.Training.2),
        pch=16,col=c("green","blue"),ylab="Separated parts of the training sample")
```

**What do you think about best way to separate the samples of residuals?**
This seems to be a good way to separate


### 1.2. Ligistic regression

Separate clusters of residuals using logistic regression with binary output from the third column and **residuals of training sample as input**

Estimate logistic regression by calling glm() function.

**Note: when we estimate logistic model we first create the data frame Logistic.Model.Data and use it in the call of glm as glm(Logistic.Output~Logistic.Input,data=Logistic.Model.Data... where Logistic.Input and Logistic.Output are the names of the data frame variables. This becomes important when we use the function predict later.**
```{r}
# Create the data frame for logistic regression
Logistic.Model.Data<-data.frame(Logistic.Output=LinearModel.Training[,3],# binary
                                Logistic.Input=EstimatedResiduals.Training)
LinearModel.Training.Logistic<-glm(Logistic.Output~Logistic.Input,data=Logistic.Model.Data,
                                   family=binomial(link=logit))
summary(LinearModel.Training.Logistic)

names(LinearModel.Training.Logistic)
```

**Interpret the summary of the model: what is the meaning and significance of coefficients.**
**significance level of interceot is not too high, however, the slop has high significance level **

Define and plot probability of selecting an observation from the first model using predict function with type="response" argument. Plot the predicted probabilities

```{r}
Predicted.Probabilities.Training<-predict(LinearModel.Training.Logistic,type="response")
plot(LinearModel.Training[,1],Predicted.Probabilities.Training) # since the predict input is the model in put it is the same as plot fitted value
plot(LinearModel.Training[,1],LinearModel.Training.Logistic$fitted.values, col="red")
```

**How can we use this graph? What does it tell us?**
**Use it as above 0.5 in one group and below 0.5 in one group, it tells us based on the residuals, which group does it most likely belongs to??**

Classify the training sample using the estimated unscrambling sequence
```{r}
# Create the unscrambling sequence for the training sample
Unscrambling.Sequence.Training.Logistic<-
  (predict(LinearModel.Training.Logistic,type="response")>.5)*1
# Create classified residuals
ClassifiedResiduals.Training.1<-EstimatedResiduals.Training
ClassifiedResiduals.Training.2<-EstimatedResiduals.Training
ClassifiedResiduals.Training.1[(Unscrambling.Sequence.Training.Logistic==0)*
                                 (1:nSample.Training)]<-NA
ClassifiedResiduals.Training.2[(Unscrambling.Sequence.Training.Logistic==1)*
                                 (1:nSample.Training)]<-NA
head(cbind(AllTraining=EstimatedResiduals.Training,
           Training1=ClassifiedResiduals.Training.1,
           Training2=ClassifiedResiduals.Training.2))
##   AllTraining Training1   Training2
## 1  0.56196024 0.5619602          NA
## 2 -0.64757362        NA -0.64757362
## 3  1.05256097 1.0525610          NA
## 4 -0.03883971        NA -0.03883971
## 5 -1.44140627        NA -1.44140627
## 6  1.45163470 1.4516347          NA
# Plot both classes of the residuals
matplot(LinearModel.Training[,1],cbind(ClassifiedResiduals.Training.1,
                                       ClassifiedResiduals.Training.2),
        pch=16,col=c("green","blue"),ylab="Classified residuals, X-axis at 0")
axis(1,pos=0)
```

**Recall what classification rule we used in the previous assignment with these data? What is the classification rule estimated by logistic regression?**

**Recall what variable is the predictor of the model. Calculate classification boundary for the models using estimated coefficients of logistic regression.**

```{r}
#Classification.Rule.Logistic # when probability of this residual belongs to one group is 50/50
## [1] 0.03906242 this is not 0, 0 is use the naive method for negative residaul belongs to one group and positive belongs to another.
Classification.Rule.Logistic = -LinearModel.Training.Logistic$coefficient[1]/LinearModel.Training.Logistic$coefficient[2]

Classification.Rule.Logistic

matplot(LinearModel.Training[,1],cbind(ClassifiedResiduals.Training.1,
                                       ClassifiedResiduals.Training.2),
        pch=16,col=c("green","blue"),ylab="Classified residuals, X-axis at the rule level")
axis(1,pos=Classification.Rule.Logistic)
```


### 1.3. Separate subsamples in the main sample using the classifier trained on the training sample

Read the main sample.
```{r}
LinearModel<-read.csv(file=paste(datapath,'ResidualAnalysisProjectData_1.csv',sep="/"),header=TRUE,sep=",")
nSample<-length(LinearModel[,1])
head(LinearModel)
```

Estimate linear model from the main sample and find the residuals.
```{r}
EstimatedLinearModel<-lm(LinearModel[,2]~LinearModel[,1])
EstimatedLinearModel$coefficients
##      (Intercept) LinearModel[, 1] 
##       0.03160231       0.79627673
EstimatedResiduals<-EstimatedLinearModel$residuals
plot(LinearModel[,1],EstimatedResiduals)
```

Define the predicted probabilities and separating sequence for the main sample.

**Note: the function predict uses the argument newdata. This argument should be a data frame with the same names of variables as the data frame used to estimate logistic model. But the residuals in this data frame are from the main sample.**
**why output is the same, is it just a place holder?**

```{r}
Unscrambling.Sequence.Logistic<-(predict(LinearModel.Training.Logistic, 
                                         newdata=data.frame(Logistic.Output=EstimatedResiduals,
                                                            Logistic.Input=EstimatedResiduals),
                                         type="response")>.5)*1 #using model built on training data
```
Estimate probability of the data classified as the first model.
Check the hypothesis p=0.5 against two-sided alternative.

```{r}
Probability<-sum(Unscrambling.Sequence.Logistic)/length(Unscrambling.Sequence.Logistic)
Probability
```

```{r}
#Run binomial test for the null hypothesis p=0.5 and two-sided alternative “p is not equal to 0.5”. Interpret the output of binom.test
binom.test(sum(Unscrambling.Sequence.Logistic),length(Unscrambling.Sequence.Logistic) , p = 0.5,
           alternative = "two.sided",
           conf.level = 0.95)
```

**What do you conclude based on the binomial test?**
**it included 0.5, so we fail to reject the null** **This method is more robust than the previous one, the previous one only works well when two groups have approximately equal number, but this model of logistic regression would work well even when the two groups don't have approximately equal number**


Classify the residuals of the main sample into 2 groups using the logistic model classifier. Plot them.
```{r}
# Create classified residuals
ClassifiedResiduals.1<-EstimatedResiduals
ClassifiedResiduals.2<-EstimatedResiduals
ClassifiedResiduals.1[(Unscrambling.Sequence.Logistic==0)*(1:nSample)]<-NA
ClassifiedResiduals.2[(Unscrambling.Sequence.Logistic==1)*(1:nSample)]<-NA
# Print first 10 rows to check
cbind(EstimatedResiduals,ClassifiedResiduals.1,ClassifiedResiduals.2)[1:10,]
##    EstimatedResiduals ClassifiedResiduals.1 ClassifiedResiduals.2
## 1         -0.20319222                    NA            -0.2031922
## 2         -1.26746423                    NA            -1.2674642
## 3          1.01249601            1.01249601                    NA
## 4         -0.68559015                    NA            -0.6855901
## 5          0.34052527            0.34052527                    NA
## 6          0.62154299            0.62154299                    NA
## 7          0.06188062            0.06188062                    NA
## 8         -0.40786555                    NA            -0.4078656
## 9         -0.33822412                    NA            -0.3382241
## 10         0.04381600            0.04381600                    NA
# Plot both classes of the residuals
matplot(LinearModel[,1],cbind(ClassifiedResiduals.1,
                              ClassifiedResiduals.2),
        pch=16,col=c("green","blue"),ylab="Classes of the main sample, X-axis at 0")
axis(1,pos=0)
```

The X axis on the plot above is located at zero level. The following graph shows it at the level of the classification rule estimated by logistic model.
```{r}
matplot(LinearModel[,1],cbind(ClassifiedResiduals.1,ClassifiedResiduals.2),
        pch=16,col=c("green","blue"),ylab="Classes of the main sample, X-axis at the rule level")
axis(1,pos=Classification.Rule.Logistic)
```

Separate the given sample into 2 subsamples using the trained logistic model.
```{r}
# Create recovered models
LinearModel1.Recovered<-LinearModel
LinearModel2.Recovered<-LinearModel
LinearModel1.Recovered[(1-Unscrambling.Sequence.Logistic)*(1:nSample),2]<-NA
LinearModel2.Recovered[Unscrambling.Sequence.Logistic*(1:nSample),2]<-NA
# Print the first 1 rows of scrambled and unscrambled samples
cbind(LinearModel,LinearModel1.Recovered,LinearModel2.Recovered)[1:10,]
##         Input     Output      Input   Output      Input     Output
## 1   3.6664327  2.7479052  3.6664327       NA  3.6664327  2.7479052
## 2  -2.5194424 -3.2420353 -2.5194424       NA -2.5194424 -3.2420353
## 3   0.6475581  1.5597337  0.6475581 1.559734  0.6475581         NA
## 4   2.4439621  1.2920823  2.4439621       NA  2.4439621  1.2920823
## 5   1.9921334  1.9584170  1.9921334 1.958417  1.9921334         NA
## 6   1.7534556  2.0493812  1.7534556 2.049381  1.7534556         NA
## 7   2.7300053  2.2673226  2.7300053 2.267323  2.7300053         NA
## 8   1.2366129  0.6084228  1.2366129       NA  1.2366129  0.6084228
## 9   1.7351840  1.0750648  1.7351840       NA  1.7351840  1.0750648
## 10  2.6600869  2.1935836  2.6600869 2.193584  2.6600869         NA
# Plot the unscrambled subsamples
matplot(LinearModel[,1],cbind(LinearModel1.Recovered[,2],LinearModel2.Recovered[,2]), type="p",col=c("green","blue"),pch=19,ylab="Separated Subsamples")
```

Now estimate the linear models for the subsamples.
```{r}
LinearModel1.Recovered.lm<-lm(LinearModel1.Recovered[,2]~LinearModel1.Recovered[,1])
LinearModel2.Recovered.lm<-lm(LinearModel2.Recovered[,2]~LinearModel2.Recovered[,1])

#Compare the results of fitting of the firstand second recovered linear model:
summary(LinearModel1.Recovered.lm)

summary(LinearModel2.Recovered.lm)
```

**Compare the summaries of the mix with the summary of the single linear model fit.**
**much higher Ad R square and lower stnadard error**

Plot the residuals estimated by a single linear model and the residuals of the unscrambled mix. Estimate standard deviations of the two samples of residuals:

```{r}
# Plot residuals
Residuals.Comparison<-cbind(Unscrambled.residuals=c(summary(LinearModel1.Recovered.lm)$residuals,summary(LinearModel2.Recovered.lm)$residuals),Single.Model.residuals=EstimatedResiduals)
matplot(Residuals.Comparison,type="p",pch=16,ylab="Residuals before and after unscrabling")
```

Estimate standard deviations
```{r}
apply(Residuals.Comparison,2,sd)
apply(Residuals.Comparison,2,var)
```

**Conclusion**

**How the sample was mixed? With what probability?**
Two different beta0, aproximately 50/50
**Was the probability significantly different from 0.5?**
no

**What were the parameters of mixed models?**
      (Intercept) LinearModel[, 1] 
       0.03160231       0.79627673

**How much we reduced variance of residuals by separating the models?**
0.70 to 0.19

Unscrumbling using logistic model gives not equivalent, but not significantly different results from our first heuristic approach.

## 2. Check Assumptions of Linear Model.
Read the data from file Week6AssignmentData.csv.

```{r}
assignmentData<-read.csv(file=paste(datapath,"Week6AssignmentData.csv",sep="/"),
                         header = TRUE,sep=",")
head(assignmentData)
```

**Are main assumptions of linear model satisfied?**
* Gaussian assumption
  **seems ok using the residual plots**
* IID assumption
** plots residual against fitted value, dw test**
* Autocorrelation function
ks test 
* Autocorrelation with lag 1

Hint. You might find assignment from week 2 useful.

```{r}
assignmentData.LinearModel <- lm(assignmentData$Output ~ assignmentData$Input)
summary(assignmentData.LinearModel)

residuals.histogram=hist(assignmentData.LinearModel$residuals)

qqnorm(assignmentData.LinearModel$residuals)
qqline(assignmentData.LinearModel$residuals)

suppressWarnings(library(fitdistrplus))

(residualsParam<-fitdistr(assignmentData.LinearModel$residuals,"normal"))

plot(density(assignmentData.LinearModel$residuals),main='Empirical PDF of residual') #check identical looks ok

ks.test(assignmentData.LinearModel$residuals,"pnorm",residualsParam$estimate[1],residualsParam$estimate[2]) ##distance of the randomly generated and theoretical, p value large enough not reject 

```


## 3. Test
```{r}
train_dat <- read.table(paste(datapath,'Week6_Test_Sample_Train.csv',sep = '/'), header=TRUE)
main_dat <- read.table(paste(datapath,'Week6_Test_Sample_Test.csv',sep = '/'), header=TRUE)
```

```{r}
lm.train_dat <- lm(train_dat$Output ~train_dat$Input)
```


```{r}
dataframe.logistic.train<-data.frame(Logistic.Output=train_dat$Selection.Sequence,# binary
                                Logistic.Input=lm.train_dat$residuals)
lm.train_dat.logistic<-glm(Logistic.Output~Logistic.Input,data=dataframe.logistic.train,
                                   family=binomial(link=logit))
summary(lm.train_dat.logistic)
```

```{r}
lm.main_dat <- lm(main_dat$Output ~main_dat$Input)
```


```{r}
Unscrambling.Sequence.Logistic2<-(predict(lm.train_dat.logistic, 
                                         newdata=data.frame(Logistic.Output=lm.main_dat$residuals,
                                                            Logistic.Input=lm.main_dat$residuals),
                                         type="response")>.5)*1 #using model built on training data
Unscrambling.Sequence.Logistic2
```



```{r}
res <- list(Unscrambling.Sequence.Logistic =  Unscrambling.Sequence.Logistic2)
#Save res to a file and upload the file using left sidebar.
write.table(res, file = paste(datapath,'result.csv',sep = '/'), row.names = F)
```


