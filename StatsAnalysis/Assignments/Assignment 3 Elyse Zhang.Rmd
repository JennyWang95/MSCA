---
title: "Assignment 3 Elyse Zhang"
output: html_document
---

Set the values of slope (a) and intercept (b) parameters. Set the sample lengths to 1,000.
```{r}
# Slope and Intercept
a<-.8; b<-.1
nSample<-1000
```

##1. Model 1

Simulate and plot Model1: input variable X ~ Norm(μ=3,σ=2.5); model residuals Eps ~ Norm(μ=0,σ=1.5)
Use set.seed(111) to simulate X.
```{r}
set.seed(111)
X = rnorm(nSample, 3, 2.5)
```

Use set.seed(1112131415) to simulate Eps.
```{r}
set.seed(1112131415)
Eps = rnorm(nSample, 0, 1.5)
```

Build the linear model with X and Eps
```{r}
Y = a * X + b + Eps
LinearModel1 = data.frame(Y, X, Eps)
head(LinearModel1)
```

Plot the linear model.
```{r}
plot(LinearModel1$X,LinearModel1$Y)
```

Plot the residuals of the model.
```{r}
plot(LinearModel1$Eps,type="l")
```

## 2. Model 2

Simulate and plot Model2: input variable X ~ Norm(μ=3,σ=2.5); model residuals Eps ~ Unif(min=−4.33,max=4.33).
Use the same realization of X as in the first model.

Use the same seed set.seed(1112131415) to simulate Eps.
```{r}
set.seed(1112131415)
Eps = runif(nSample, -4.33, 4.33)
```

Build the linear model with X and Eps
```{r}
Y = a * X + b + Eps
LinearModel2 = data.frame(Y, X, Eps)
head(LinearModel2)
```

Plot the linear model.
```{r}
plot(LinearModel2$X,LinearModel2$Y)
```

Plot the residual.
```{r}
plot(LinearModel2$Eps,type="l")
```

## 3. Model 3

Simulate and plot Model3: input variable X ~ Norm^(μ=3,σ=2.5); model residuals Eps ~ Cauchi*(location=0,scale=0.3).
Use the same realization of X as in the first model.
Use the same seed set.seed(1112131415) to simulate Eps.
Plot the residuals of the model.

Use the same seed set.seed(1112131415) to simulate Eps.
```{r}
set.seed(1112131415)
Eps = rcauchy(nSample, location = 0, scale = 0.3)
```

Build the linear model with X and Eps
```{r}
Y = a * X + b + Eps
LinearModel3 = data.frame(Y, X, Eps)
head(LinearModel3)
```

Plot the linear model.
```{r}
plot(LinearModel3$X,LinearModel3$Y)
```

Plot the residual.
```{r}
plot(LinearModel3$Eps,type="l")
```

Estimate the standard deviation of the residuals.
```{r}
sd(LinearModel3$Eps)
```

Generate another 5 samples of residuals without any seed specification and estimate standard deviations for each of them.
```{r}
Eps1<-rcauchy(n=nSample,location=0,scale=.3)
Eps2<-rcauchy(n=nSample,location=0,scale=.3)
Eps3<-rcauchy(n=nSample,location=0,scale=.3)
Eps4<-rcauchy(n=nSample,location=0,scale=.3)
Eps5<-rcauchy(n=nSample,location=0,scale=.3)
c(sd(Eps1),sd(Eps2),sd(Eps3),sd(Eps4),sd(Eps5))
```

Note the irregularity of standard deviations from sample to sample.
**How do you interpret this observation?**
**When the Eps is Cauchy distribution, the standard devieations are very small in most cases (less than 30). However, there are extreme values such as 300 or larger occationally.**

##4. Model 4

Simulate and plot Model4: input variable X ~ Norm(μ=3,σ=2.5); model residuals Eps ~ a heteroscedastic process.
Use the same realization of X as in the first model.
Use the same seed.
Plot the residuals of the model.

Heteroscedasticity means that even though the model residuals Eps may be generated by a normal distribution, the standard deviation parameters for different sub samples are different.

Create the process of standard deviations in which the first 50 observations have sigma=2, followed by 75 observations with sigma=3.4, followed by 75 observations with sigma=0.8 and concluded by 50 observations with sigma=2.6.

Plot the trajectory of standard deviations of total length nSample=1000.

```{r}
sd.Values<-c(2,3.4,.8,2.6)
sd.process<-rep(c(rep(sd.Values[1],50),
                  rep(sd.Values[2],75),
                  rep(sd.Values[3],75),
                  rep(sd.Values[4],50)),
            4) # repeat the process 4 times
            
plot(sd.process,type="l")
```

Simulate the linear model residuals Eps with changing standard deviations.

```{r}
set.seed(1112131415);
Eps<-rnorm(nSample)*sd.process #standard normal distribution times sd process number
```

Plot the residuals.
```{r}
plot(Eps,type="l")
```

Observe how heteroscedasticity transforms normal distribution into leptokurtic distribution

```{r}
Xvariable<-(100*floor(min(Eps))):(100*ceiling(max(Eps))) 
Xvariable<-Xvariable/100 #setting lower and higher x-axis limits with round ups of min and maximum Eps
# Plot the sample distribution and the theo. distribution
plot(Xvariable,dnorm(Xvariable,mean=mean(Eps),sd=sd(Eps)),type="l",
      ylim=c(0,.3),col="black",ylab="Distribution of Eps",xlab="")
lines(density(Eps),col="red")
```

Generate LinearModel4.
Plot it.
```{r}
Y<-a*X+b+Eps
LinearModel4<-as.data.frame(cbind(Y=Y,X=X))
plot(LinearModel4$X,LinearModel4$Y)

```

##5. Effect of Residual Distribution on Correlation
Calculate the theoretical ρ2 for the “correct model” which is LinearModel1. From MathChallenge_MethodOfMomentsForLinearModel.mp4 

```{r}
set.seed(1112131415)
Eps = rnorm(nSample, 0, 1.5)
```


```{r}
#Theoretical Rho^2
sd.X = 2.5
sd.Eps = 1.5
Theoretical.Rho.Squared<-(a*sd.X)^2/((a*sd.X)^2+sd.Eps^2)
Theoretical.Rho.Squared
```

And compare with the estimated ρ2
```{r}
c(cor(LinearModel1$X,LinearModel1$Y)^2,
  cor(LinearModel2$X,LinearModel2$Y)^2,
  cor(LinearModel3$X,LinearModel3$Y)^2,
  cor(LinearModel4$X,LinearModel4$Y)^2)
```

**How do you interpret the results?**
**When the error is normally distributed, the linear model is correct thus the correlation is the largest. When the error distribution is cauchy distributed, there is barely any correlation between Y and X due to the large irregularity.  When the error is uniformly distirbuted or normal but when through a heteroscedastic process, although the correlation between X and Y are not too small, around 0.4, they are not correct linear models, but merely resembles one.**

## 6. Estimation of Linear Model
Estimate parameters a,b,σ using the function lm()
```{r}
m1<-lm(Y~X,data=LinearModel1)
summary(m1)
names(summary(m1))
summary(m1)$r.squared
summary(m1)$coeff
summary(m1)$sigma^2
var(summary(m1)$residuals)
```

**Note the difference between summary(m1)$sigma^2 and var(summary(m1)$residuals)**
The two estimates are reconciled by:
```{r}
var(summary(m1)$residuals)*999/998
```

Estimate the same parameters using the method of moments directly.

Build the linear model with X and Eps
```{r}
aEstimate = cov(X, LinearModel1$Y)/var(X) # both sides should times N-1/N, but they are cancelled
bEstimate = mean(LinearModel1$Y)-aEstimate*mean(X)
sigmaEstimate=sqrt((var(LinearModel1$Y)-aEstimate^2*var(X))*(nSample-1)/nSample) # why it does not need the N-1/N what about the previous one
#sigmaEstimate=sqrt(sum((LinearModel1$Y-aEstimate*X-bEstimate)^2)/(nSample-1))

c(aEstimate,bEstimate,sigmaEstimate)
```

**Reconcile sigmaEstimate with m1$sigma.**
```{r}
c(sigmaMetodMoments=sigmaEstimate,sigmaLinearModel=summary(m1)$sigma)
```

## 7. Fit lm() to the the Rest of Linear Models
**Compare the differences between the assumptions of the 4 models and tell how they change the model behavior and estimated parameters.**
```{r}
m2<-lm(Y~X,data=LinearModel2)
m3<-lm(Y~X,data=LinearModel3)
m4<-lm(Y~X,data=LinearModel4)
```

```{r}
summary(m2)$coeff
summary(m2)$sigma
summary(m2)$r.squared
summary(m2)$df
summary(m3)$coeff
summary(m3)$sigma
summary(m3)$r.squared
summary(m3)$df
summary(m4)$coeff
summary(m4)$sigma
summary(m4)$r.squared
summary(m4)$df
```

**Observations**
* **all models seem to have similar slope between 0.74-0.83, but I imagine if we change seeds for the model3 error distribution, we could see very different slope.**
* **model3 with cauchy error has largest intercept whileas others have similar intercept around 0.17. Again with different seed, the intercept can be very different.**
* **model3 also has largest sigma of 18.99 due to the extreme erros in the distribution. Model2 and Model4 has similar sigma around 2.4, it's slightly higher than model1's 1.5.**
* **correlation r squared is discussed in Section 5.**
* **All models are expected to have same degrees of freedom and they do.**


## Test 3

```{r}
dataPath<-"~/Documents/UChicago/Courses/Statistical Analysis/Assignments/Week 3"
dat <- read.table(paste(dataPath,'Week3_Test_Sample.csv',sep = '/'), header=TRUE)
```

```{r}
m5<-lm(Y~X,data=dat)
summary(m5)
mean(m5$residuals)
```

